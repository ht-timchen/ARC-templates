
@ARTICLE{Bau2017-jt,
  title     = "Learnable programming: blocks and beyond",
  author    = "Bau, David and Gray, Jeff and Kelleher, Caitlin and Sheldon,
               Josh and Turbak, Franklyn",
  abstract  = "New blocks frameworks open doors to greater experimentation for
               novices and professionals alike.",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  volume    =  60,
  number    =  6,
  pages     = "72--80",
  month     =  may,
  year      =  2017,
  address   = "New York, NY, USA"
}


@INPROCEEDINGS{Ens2017-iu,
  title     = "Ivy: Exploring Spatially Situated Visual Programming for
               Authoring and Understanding Intelligent Environments",
  booktitle = "Proceedings of the 43rd Graphics Interface Conference",
  author    = "Ens, Barrett and Anderson, Fraser and Grossman, Tovi and Annett,
               Michelle and Irani, Pourang and Fitzmaurice, George",
  abstract  = "The availability of embedded, digital systems has led to a
               multitude of interconnected sensors and actuators being
               distributed among smart objects and built environments.
               Programming and understanding the behaviors of such systems can
               be challenging given their inherent spatial nature. To explore
               how spatial and contextual information can facilitate the
               authoring of intelligent environments, we introduce Ivy, a
               spatially situated visual programming tool using immersive
               virtual reality. Ivy allows users to link smart objects, insert
               logic constructs, and visualize real-time data flows between
               real-world sensors and actuators. Initial feedback sessions show
               that participants of varying skill levels can successfully
               author and debug programs in example scenarios.",
  publisher = "Canadian Human-Computer Communications Society",
  pages     = "156--162",
  series    = "GI '17",
  month     =  jan,
  year      =  2017,
  address   = "Waterloo, CAN",
  keywords  = "visual programming language, spatial interaction, mixed reality,
               internet of thing, Virtual reality, immersive analytics",
  location  = "Edmonton, Alberta, Canada"
}

@ARTICLE{Shneiderman2020-dt,
  title     = "{Human-Centered} Artificial Intelligence: Reliable, Safe \&
               Trustworthy",
  author    = "Shneiderman, Ben",
  abstract  = "ABSTRACTWell-designed technologies that offer high levels of
               human control and high levels of computer automation can
               increase human performance, leading to wider adoption. The
               Human-Centered Artificial Intelligence (HCAI) framework
               clarifies how to (1) design for high levels of human control and
               high levels of computer automation so as to increase human
               performance, (2) understand the situations in which full human
               control or full computer control are necessary, and (3) avoid
               the dangers of excessive human control or excessive computer
               control. The methods of HCAI are more likely to produce designs
               that are Reliable, Safe \& Trustworthy (RST). Achieving these
               goals will dramatically increase human performance, while
               supporting human self-efficacy, mastery, creativity, and
               responsibility.",
  journal   = "International Journal of Human--Computer Interaction",
  publisher = "Taylor \& Francis",
  volume    =  36,
  number    =  6,
  pages     = "495--504",
  month     =  apr,
  year      =  2020
}

@INPROCEEDINGS{Zhang2020-uo,
  title     = "{FlowMatic}: An Immersive Authoring Tool for Creating
               Interactive Scenes in Virtual Reality",
  booktitle = "Proceedings of the 33rd Annual {ACM} Symposium on User Interface
               Software and Technology",
  author    = "Zhang, Lei and Oney, Steve",
  abstract  = "Immersive authoring is a paradigm that makes Virtual Reality
               (VR) application development easier by allowing programmers to
               create VR content while immersed in the virtual environment. In
               this paradigm, programmers manipulate programming primitives
               through direct manipulation and get immediate feedback on their
               program's state and output. However, existing immersive
               authoring tools have a low ceiling; their programming primitives
               are intuitive but can only express a limited set of static
               relationships between elements in a scene. In this paper, we
               introduce FlowMatic, an immersive authoring tool that raises the
               ceiling of expressiveness by allowing programmers to specify
               reactive behaviors---behaviors that react to discrete events
               such as user actions, system timers, or collisions. FlowMatic
               also introduces primitives for programmatically creating and
               destroying new objects, for abstracting and re-using
               functionality, and for importing 3D models. Importantly,
               FlowMatic uses novel visual representations to allow these
               primitives to be represented directly in VR. We also describe
               the results of a user study that illustrates the usability
               advantages of FlowMatic relative to a 2D authoring tool and we
               demonstrate its expressiveness through several example
               applications that would be impossible to implement with existing
               immersive authoring tools. By combining a visual program
               representation with expressive programming primitives and a
               natural User Interface (UI) for authoring programs, FlowMatic
               shows how programmers can build fully interactive virtual
               experiences with immersive authoring.",
  publisher = "Association for Computing Machinery",
  pages     = "342--353",
  series    = "UIST '20",
  month     =  oct,
  year      =  2020,
  address   = "New York, NY, USA",
  keywords  = "immersive authoring, visual programming, virtual reality",
  location  = "Virtual Event, USA"
}

@INPROCEEDINGS{Wang2020-lv,
  title     = "{CAPturAR}: An Augmented Reality Tool for Authoring
               {Human-Involved} {Context-Aware} Applications",
  booktitle = "Proceedings of the 33rd Annual {ACM} Symposium on User Interface
               Software and Technology",
  author    = "Wang, Tianyi and Qian, Xun and He, Fengming and Hu, Xiyun and
               Huo, Ke and Cao, Yuanzhi and Ramani, Karthik",
  abstract  = "Recognition of human behavior plays an important role in
               context-aware applications. However, it is still a challenge for
               end-users to build personalized applications that accurately
               recognize their own activities. Therefore, we present CAPturAR,
               an in-situ programming tool that supports users to rapidly
               author context-aware applications by referring to their previous
               activities. We customize an AR head-mounted device with multiple
               camera systems that allow for non-intrusive capturing of user's
               daily activities. During authoring, we reconstruct the captured
               data in AR with an animated avatar and use virtual icons to
               represent the surrounding environment. With our visual
               programming interface, users create human-centered rules for the
               applications and experience them instantly in AR. We further
               demonstrate four use cases enabled by CAPturAR. Also, we verify
               the effectiveness of the AR-HMD and the authoring workflow with
               a system evaluation using our prototype. Moreover, we conduct a
               remote user study in an AR simulator to evaluate the usability.",
  publisher = "Association for Computing Machinery",
  pages     = "328--341",
  series    = "UIST '20",
  month     =  oct,
  year      =  2020,
  address   = "New York, NY, USA",
  keywords  = "ubiquitous computing, embodied authoring, context-aware
               application, in-situ authoring, end-user programming tool,
               augmented reality",
  location  = "Virtual Event, USA"
}

@INPROCEEDINGS{MacIntyre2004-wd,
  title     = "{{DART}: a toolkit for rapid design exploration of augmented
               reality experiences}",
  booktitle = "Proceedings of the 17th annual {ACM} symposium on User interface
               software and technology",
  author    = "MacIntyre, Blair and Gandy, Maribeth and Dow, Steven and Bolter,
               Jay David",
  abstract  = "In this paper, we describe The Designer's Augmented Reality
               Toolkit (DART). DART is built on top of Macromedia Director, a
               widely used multimedia development environment. We summarize the
               most significant problems faced by designers working with AR in
               the real world, and discuss how DART addresses them. Most of
               DART is implemented in an interpreted scripting language, and
               can be modified by designers to suit their needs. Our work
               focuses on supporting early design activities, especially a
               rapid transition from story-boards to working experience, so
               that the experiential part of a design can be tested early and
               often. DART allows designers to specify complex relationships
               between the physical and virtual worlds, and supports 3D
               animatic actors (informal, sketch-based content) in addition to
               more polished content. Designers can capture and replay
               synchronized video and sensor data, allowing them to work
               off-site and to test specific parts of their experience more
               effectively.",
  publisher = "Association for Computing Machinery",
  pages     = "197--206",
  series    = "UIST '04",
  month     =  oct,
  year      =  2004,
  address   = "New York, NY, USA",
  keywords  = "mixed reality, augmented reality, storyboards, capture/replay,
               design environments, animatics",
  location  = "Santa Fe, NM, USA"
}

@INPROCEEDINGS{Yeh2009-th,
  title     = "{Sikuli: using {GUI} screenshots for search and automation}",
  booktitle = "Proceedings of the 22nd annual {ACM} symposium on User interface
               software and technology",
  author    = "Yeh, Tom and Chang, Tsung-Hsiang and Miller, Robert C",
  abstract  = "We present Sikuli, a visual approach to search and automation of
               graphical user interfaces using screenshots. Sikuli allows users
               to take a screenshot of a GUI element (such as a toolbar button,
               icon, or dialog box) and query a help system using the
               screenshot instead of the element's name. Sikuli also provides a
               visual scripting API for automating GUI interactions, using
               screenshot patterns to direct mouse and keyboard events. We
               report a web-based user study showing that searching by
               screenshot is easy to learn and faster to specify than keywords.
               We also demonstrate several automation tasks suitable for visual
               scripting, such as map navigation and bus tracking, and show how
               visual scripting can improve interactive help systems previously
               proposed in the literature.",
  publisher = "Association for Computing Machinery",
  pages     = "183--192",
  series    = "UIST '09",
  month     =  oct,
  year      =  2009,
  address   = "New York, NY, USA",
  keywords  = "image search, automation, online help",
  location  = "Victoria, BC, Canada"
}


@inproceedings{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL-HLT},
  year={2019}
}

@inproceedings{harnessing_prior,
    title = "Harnessing Deep Neural Networks with Logic Rules",
    author = "Hu, Zhiting  and
      Ma, Xuezhe  and
      Liu, Zhengzhong  and
      Hovy, Eduard  and
      Xing, Eric",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1228",
    doi = "10.18653/v1/P16-1228",
    pages = "2410--2420",
}

@incollection{drum,
title = {DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs},
author = {Sadeghian, Ali and Armandpour, Mohammadreza and Ding, Patrick and Wang, Daisy Zhe},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. dAlch\'{e}-Buc and E. Fox and R. Garnett},
pages = {15321--15331},
year = {2019},
publisher = {Curran Associates, Inc.}
}

@ARTICLE{rep_learning,
author={Y. {Bengio} and A. {Courville} and P. {Vincent}},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Representation Learning: A Review and New Perspectives},
year={2013},
volume={35},
number={8},
pages={1798-1828},
keywords={artificial intelligence;data structures;probability;unsupervised learning;representation learning;machine learning algorithms;data representation;AI;unsupervised feature learning;probabilistic models;autoencoders;manifold learning;geometrical connections;density estimation;Learning systems;Machine learning;Abstracts;Feature extraction;Manifolds;Neural networks;Speech recognition;Deep learning;representation learning;feature learning;unsupervised learning;Boltzmann machine;autoencoder;neural nets;Algorithms;Artificial Intelligence;Humans;Neural Networks (Computer)},
doi={10.1109/TPAMI.2013.50},
ISSN={1939-3539},
month={Aug},}